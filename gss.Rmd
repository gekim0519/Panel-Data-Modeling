---
title: "Panel Data Modeling"
author: "Sara Kim"
date: "5/30/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
```

```{r, message=FALSE, warning=FALSE}
gss_all_original = readr::read_csv('./data/gss-all-panels-06-14.csv')

# Instead of removing ".d", ".i", ".n" I will alter them to NAs.
gss_all = readr::read_csv('./data/gss-all-panels-06-14.csv', na = c(".d", ".i", ".n")) %>%
  janitor::remove_empty_cols()
```

So in `gss_all`, I changed all ".d", ".i", ".n" to NAs and removed columns that were all NAs.

changing orders in levels of `tvhours`
```{r}
gss_all$tvhours = gss_all$tvhours %>% factor(levels = c("0",  "1", "2", "3", "4",  "5",  "6",  "7",  "8",  "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "23", "24"))
```

## Project Idea 

When people express less confidence in a certain institution, do we see them engaging with that institution less too?  Confidence in institutions has dropped dramatically, with implications for civil society, social capital and public trust.  Using two-way fixed effects models (that control for stable, time-invariant characteristics of individuals), I find some evidence that when people's confidence in certain institutions changes, so too does their participation and commitment to those institutions.  This is consistent with a theory that people’s opinions and actions share some common basis and are not “uncoupled” from each other.  This appears true for confidence in TV, religion, business and unions, but not so for confidence in the press or education.  Using random intercept cross-lagged SEMs, we can also test whether the relationship is bi-directional or only goes one-way.    

Let's first explore the relationship between confidence in TV (`contv`) and their daily TV watching habits (`tvhours`).

The variables we are going to focus is these two:

`CONTV`: I am going to name some institutions in this country. As far as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? 

`TVHOURS`: On the average day, about how many hours do you personally watch television?

## Data Exploration

First of all, let's look at the size of our data.

```{r}
dim(gss_all)
unique(gss_all$idnum) %>% length()
```

There are data from 6067 participants.

### Frequency table

```{r}
gss_all$tvhours %>% 
  table() %>%
  as.data.frame(col.names = values) %>%
  rename(., values = .) %>%
  mutate(Percent = Freq/sum(Freq)) %>%
  knitr::kable()


gss_all$contv %>% 
  table() %>%
  as.data.frame(col.names = values) %>%
  rename(., values = .) %>%
  mutate(Percent = Freq/sum(Freq)) %>%
  knitr::kable()

```
I made Frequency tables for the two variables. Let's see summary statistics for `tvhours` by considering `tvhours` as a numeric variable.

```{r}
gss_all$tvhours %>% 
  as.numeric() %>%
  Hmisc::describe()
```


I will plot the two variables for each `panelwave` and see if the distribution changes depending on the wave.

```{r}
 gss_all %>%
  ggplot(aes(x=contv)) +
  geom_bar(stat="count") + 
  facet_grid(. ~ panelwave)

 gss_all %>%
  ggplot(aes(x=tvhours)) +
  geom_bar(stat="count") + 
  facet_grid(. ~ panelwave)
```

Seems like there is no apparent difference in the trends among the three panels.

```{r}
set.seed(100) 

gss_all %>%
  filter(idnum %in% (gss_all$idnum %>% sample(1000))) %>%
  mutate(idnum = factor(idnum)) %>%
  ggplot(aes(x = panelwave, y = tvhours, group = idnum, color = idnum)) +
  geom_point() + geom_line(alpha = 0.5) + theme(legend.position="none")
```

Just sampled 1000 participants to see if there is a pattern. 

### Correlation

Below is a correlation matrix between `contv` and `tvhours` for the first wave. I removed non-numeric values and altered the variables to be numeric to see the relationship.

```{r}
gss_all %>%
  mutate(contv = as.numeric(contv),
         tvhours = as.numeric(tvhours)) %>%
  select(tvhours, contv) %>%
  cor(use="complete.obs")

gss_all %>%
  filter(panelwave == 1) %>%
  mutate(contv = as.numeric(contv),
         tvhours = as.numeric(tvhours)) %>%
  select(tvhours, contv) %>%
  cor(use="complete.obs")

library(purrr)
gss_numeric = map_df(gss_all, as.numeric) 
gss_numeric_p1 = gss_numeric %>%
  filter(panelwave == 1)

panel1_cor = cor(gss_numeric_p1, use="p")

contv_cor = panel1_cor  %>%
  as.data.frame() %>%
  select(contv) %>%
  rownames_to_column('var') %>% 
  filter(contv > 0.2)

# negatively corr check
# check top ten? five?
# try ten% 
# put in the y var in amelia

contv_cor

panel1_cor  %>%
  as.data.frame() %>%
  select(contv) %>%
  rownames_to_column('var') %>% 
  filter(contv < -0.25)
```
Above is a pearson correlation coefficient with `contv` with all the other variables. I have filtered out those that are above 0.2.

Should take out some variables. Like idnum, panelwave, ...

Lot of relate vars have cor value of 1. Why?

```{r}
gss_numeric_p1 %>%
  select(contv_cor$var) %>%
  select(starts_with('rel'))
```

Seems like these columns are very sparse.
Should I remove sparse columns?

```{r}
#library(lsr)
#corr package
library(corrr)
#gss_all %>% filter(panelwave == 1) %>% correlate() %>% focus(contv) %>% arrange(tvhours)
```

```{r}
library(Amelia)

gss_numeric_p1 %>%
  select(contv_cor$var) %>%
  is.na() %>%
  as.data.frame() %>%
  map(sum)

# remove 50%? na vars
# look at sparse vars
```
I counted missings in each variables. Considering there are 6067 rows, most of the variables are very sparse columns.